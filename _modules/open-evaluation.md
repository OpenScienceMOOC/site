---
layout: page-centered
title: "Open Evaluation"
---

## Rationale
Concurrent with broader developments in Open Science and increased transparency 
in research, Open Peer Review is a complex, and rapidly evolving topic. 
Alongside this, more diverse criteria of research evaluation beyond traditional 
methods are emerging, and with these come a range of practical, ethical, and 
social factors to consider. This module will provide insight into current 
developments in Open Peer Review and research evaluation.

## Learning outcomes

* The researcher will be able to describe the history of peer review in the 
  context of scholarly publishing, the criticisms levied against ‘traditional’ 
  peer review, and the ongoing developments with Open Peer Review.
* The researcher will be able to use a range of post-publication review, 
  commenting, and annotation services.
* The researcher will be able to describe the issues associated with the use of 
  ‘traditional’ metrics in research evaluation, and the role that peer evaluation 
  and ‘next-generation’ metrics (or ‘altmetrics’) play in this.
* The researcher will be able to use a range of services to build and demonstrate 
  their personal research impact profile, both quantitatively and qualitatively.
* The researcher will become familiar with the relevant criteria for research 
  evaluation to them, and be able to have a critical discussion about them with 
  their colleagues and those who drafted them.

[_Resources_](http://opensciencemooc.eu/resources/#seven)

## Resources

### Tools

- [Peer review report template](https://www.authorea.com/templates/peer_review_report_template), Authorea
- [Eigenfactor project](http://www.eigenfactor.org/index.php)
- [Publons Academy](https://publons.com/community/academy)
- [Metrics Toolkit](http://www.metrics-toolkit.org/)
- [Open Badges](https://openbadges.org/)

### Research Articles and Reports

- [Why the impact factor of journals should not be used for evaluating research](http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC2126010&blobtype=pdf), Seglen, 1997
- [Effect of open peer review on quality of reviews and on reviewers' recommendations: a randomised trial](http://www.bmj.com/content/318/7175/23), van Rooyen et al., 1999
- [A Reliability-Generalization Study of Journal Peer Reviews: A Multilevel Meta-Analysis of Inter-Rater Reliability and Its Determinants](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0014331), Bornmann et al., 2010
- [Effect on peer review of telling reviewers that their signed reviews might be posted on the web: randomised controlled trial](http://www.bmj.com/content/341/bmj.c5729), van Rooyen et al., 2010
- [Open peer review: A randomised controlled trial](https://www.cambridge.org/core/journals/the-british-journal-of-psychiatry/article/open-peer-review-a-randomised-controlled-trial/1F81447FC67B3BAFDCCCCE82B6C7A187), Walsh et al., 2010
- [Deep impact: unintended consequences of journal rank](https://www.frontiersin.org/articles/10.3389/fnhum.2013.00291/full), Brembs et al., 2013
- [Excellence by Nonsense: The Competition for Publications in Modern Science](https://link.springer.com/chapter/10.1007/978-3-319-00026-8_3), Binswanger, 2014
- [Attention! A study of open access vs non-open access articles](https://figshare.com/articles/Attention_A_study_of_open_access_vs_non_open_access_articles/1213690), Adie, 2014
- [Publishing: Credit where credit is due](http://www.nature.com/news/publishing-credit-where-credit-is-due-1.15033), Allen et al., 2014
- [The Metric Tide](https://responsiblemetrics.org/the-metric-tide/) report, Wilsdon et al., 2015
- [Grand challenges in altmetrics: heterogeneity, data quality and dependencies](https://arxiv.org/abs/1603.04939), Haustein, 2016
- [Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency](http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002456), Kidwell et al., 2016
- [A framework to monitor open science trends in the EU](https://www.oecd.org/sti/063%20-%20OECD%20Blue%20Sky%202016_Open%20Science.pdf), Smith et al., 2016
- [Peer Review Survey 2015: Key Findings](http://publishingresearchconsortium.com/index.php/134-news-main-menu/prc-peer-review-survey-2015-key-findings/172-peer-review-survey-2015-key-findings), Mark Ware Consulting, 2016
- [Point of View: How open science helps researchers succeed](https://elifesciences.org/articles/16800), McKiernan et al., 2016
- [Peer Review Quality and Transparency of the Peer-Review Process in Open Access and Subscription Journals](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0147913), Wicherts, 2016
- [Next-generation metrics: Responsible metrics and evaluation for open science](https://ec.europa.eu/research/openscience/pdf/report.pdf), European Commission, 2017
- [Evaluation of Research Careers fully acknowledging Open Science Practices: Rewards, incentives and/or recognition for researchers practicing Open Science](https://ec.europa.eu/research/openscience/pdf/os_rewards_wgreport_final.pdf), European Commission, 2017
- [Research: Gender bias in scholarly peer review](https://elifesciences.org/articles/21718), Helmer et al., 2017
- ["Excellence R Us": university research and the fetishisation of excellence](https://www.nature.com/articles/palcomms2016105), Moore et al., 2017
- [Metrics for openness](https://researchcommons.waikato.ac.nz/handle/10289/10842), Nichols and Twidale, 2017
- [What is open peer review? A systematic review](https://f1000research.com/articles/6-588/v2), Ross-Hellauer, 2017
- [Survey on open peer review: Attitudes and experience amongst editors, authors and reviewers](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0189311), Ross-Hellauer et al., 2017
- [A multi-disciplinary perspective on emergent and future innovations in peer review](https://f1000research.com/articles/6-1151/v3), Tennant et al., 2017
- [Reviewer bias in single- versus double-blind peer review](http://www.pnas.org/content/114/48/12708), Tomkins et al., 2017
- [Prestigious science journals struggle to reach even average reliability](https://www.frontiersin.org/articles/10.3389/fnhum.2018.00037/full), Brembs, 2018
- [Making research evaluation more transparent: Aligning research philosophy, institutional values, and reporting](https://psyarxiv.com/48qux/), Dougherty et al., 2018
- [Research excellence indicators: time to reimagine the 'making of'?](https://academic.oup.com/spp/advance-article/doi/10.1093/scipol/scy007/4858431), Ferretti et al., 2018
- [The Journal Impact Factor: A brief history, critique, and discussion of adverse effects](https://arxiv.org/abs/1801.08992), Lariviere and Sugimoto, 2018

### Key posts

- [Six essential reads on peer review](http://asapbio.org/six-essential-reads-on-peer-review), ASAPbio
- [Peer reviews are open for registering at Crossref](https://www.crossref.org/blog/peer-reviews-are-open-for-registering-at-crossref/), Jennifer Lin
- [Why we don't sign our peer reviews](http://www.molecularecologist.com/2014/04/why-we-dont-sign/), Jeremy Yoder
- [The Fractured Logic of Blinded Peer Review in Journals](http://blogs.plos.org/absolutely-maybe/2017/10/31/the-fractured-logic-of-blinded-peer-review-in-journals/), Hilda Bastian
- [The peer review process: challenges and progress](https://www.editage.com/insights/the-peer-review-process-challenges-and-progress), Irene Hames
- [Responsible metrics: Where it's at?](https://thebibliomagician.wordpress.com/2018/02/16/responsible-metrics-where-its-at/), Lizzie Gadd
- [Goodhart's Law and why measurement is hard](https://www.ribbonfarm.com/2016/06/09/goodharts-law-and-why-measurement-is-hard/), David Manheim
- [Academe's prestige problem: We're all complicit in perpetuating a rigged system](https://www.chronicle.com/article/Academe-s-Prestige-Problem/241432), Maximillian Alvarez
- [Let's move beyond the rhetoric: it's time to change how we judge research](https://www.nature.com/articles/d41586-018-01642-w) ,Stephen Curry
- [Blockchain offers a true route to a scholarly commons](https://www.researchresearch.com/news/article/?articleId=1373351), Lambert Heller

### Other

- [Metrics and Research Assessment](https://www.scienceopen.com/search#collection/78c15291-27e3-493a-99ec-7e5a00387745), ScienceOpen collection
- [The Open Access Citation Advantage](https://www.scienceopen.com/search#collection/996823e0-8104-4490-b26a-f2f733f810fb), ScienceOpen collection
- [Citation Behaviour and Practice](https://www.scienceopen.com/search#collection/2d601af5-aa90-4c63-9a11-85f2dc768868), ScienceOpen collection
- [Scholarly Publication Practices and Impact Factor Calculation and Manipulation](https://www.scienceopen.com/search#collection/e4870106-eea5-4ba3-88cf-e769c7d49ebe),
 ScienceOpen collection
- [Peer Review in the Age of Open Science](https://www.slideshare.net/OpenAIRE_eu/peer-review-in-the-age-of-open-science), Tony Ross-Hellauer, 2017
- [The San Francisco Declaration on Research Assessment](http://www.ascb.org/dora/) (DORA)
- [The Leiden Manifesto](http://www.leidenmanifesto.org/)
- [The Humane Metrics Initiative](http://humetricshss.org/about/)
- [Open Research Badges](https://openresearchbadges.org/)
- [OpenUpHub must reads](https://www.openuphub.eu/review/must-reads)
- [NISO Alternative Assessment Metrics (Altmetrics) Initiative](http://www.niso.org/standards-committees/altmetrics)
- [Snowball Metrics](https://www.snowballmetrics.com/), standardized research metrics
