---
title: "It is time to fix research evaluation"
author: "Jon Tennant"
date: "2019-10-15"
layout: post
categories: evaluation
---

Joint post by [Paola Masuzzo](https://twitter.com/pcmasuzzo/), [Lonni Besançon](https://twitter.com/lonnibesancon), and [Jon Tennant](https://twitter.com/Protohedgehog/).

As children, we are taught “Don’t judge a book by its cover”. Yet, when it comes to evaluation of scholarly research, this is often exactly what we do. In [1997](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2126010/pdf/9056804.pdf), an article was published entitled “Why the impact factor of journals should not be used for evaluating research”. Yet, twenty years later, this metric is still pervasively used for research assessment, and closely tied to the quality of articles and their authors, as has been recently reported for [North America](https://elifesciences.org/articles/47338) and across parts of [Europe](https://www.timeshighereducation.com/news/most-european-campuses-use-journal-impact-factor-judge-staff). 

We do not understand this. There is no theoretical, ethical, or empirical basis for this system of evaluation. The three of us are early career researchers, and we are deeply concerned. It should not be up to us to compensate for the lack of leadership and responsibility demonstrated by senior academics, who should be doing research evaluation with the same rigour they perform their research.

Why can the supposed intellectual elite of this planet not get past this? “What is the alternative?”, we hear you cry oh clichéd professor. Great advances in science were made before this system was even considered and it is therefore irrational to believe that this is the only way to expand knowledge. Apathy is unacceptable in the face of mounting evidence of the problems this causes, including increasing [questionable research practices](https://journals.sagepub.com/doi/10.1177/1948550618790227) and [retractions](https://www.embopress.org/doi/10.1038/sj.embor.7401143), and a [mental health crisis](https://www.nature.com/articles/d41586-019-01492-0) within the publish or perish paradigm.

Science is not a competition but an iterative and collaborative process. We need to fix the research evaluation system, now, and make it compatible with the way we perform and value research. This includes the adoption of a humane-oriented model that supports slow, robust, and inclusive participatory research, and values a [diversity of research outputs and processes](https://www.timeshighereducation.com/news/ref-must-accommodate-more-diverse-outputs-says-study). If we do this, we can create a healthier research culture for everyone, and look back on this time in our history with pride.
